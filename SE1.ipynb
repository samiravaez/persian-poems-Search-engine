{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index:'persian_poems_se1' is deleted\n"
     ]
    }
   ],
   "source": [
    "# Search engine SE1(----) with ElasticSearch\n",
    "# //The files' paths are compatible with Ubuntu 20.04 and the code is just tested in this OS\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import glob\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "\n",
    "directory = 'Poems/*'\n",
    "query_directory = 'Queries/*'\n",
    "stop_words_path = 'Stopwords/Stopwords.txt'\n",
    "relevant_docs_path = 'RelevanceAssesment/RelevanceAssesment.txt'\n",
    "\n",
    "# read stop words\n",
    "with open(stop_words_path, 'r', encoding=\"utf8\") as s_file:\n",
    "    stopWords = [word for line in s_file for word in line.split()]\n",
    "\n",
    "\n",
    "def connect():\n",
    "    es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    if es.ping():\n",
    "        return es\n",
    "    else:\n",
    "        print(\"Unable to connect to elasticsearch!\")\n",
    "\n",
    "\n",
    "def get_files_in_dir(directory):\n",
    "    file_list = []\n",
    "    for filename in glob.glob(directory, recursive=True):\n",
    "        file_list += [filename]\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def create_index(index_name):\n",
    "    elastic = Elasticsearch()\n",
    "    # if not elastic.indices.exists(index_name):\n",
    "    elastic.indices.create(index=index_name, ignore=[400, 404])\n",
    "    # else:\n",
    "    #     print('The index already exists..')\n",
    "\n",
    "\n",
    "def delete_index(index_names):\n",
    "    es = connect()\n",
    "    for element in index_names:\n",
    "        es.indices.delete(index=element, ignore=[400, 404])\n",
    "        print(\"The index:'\" + element + \"' is deleted\")\n",
    "\n",
    "\n",
    "def get_text_file_data(file):\n",
    "    data = []\n",
    "    with open(file, encoding=\"utf8\") as file:\n",
    "      for line in file:\n",
    "        data += [str(line)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def text_files_parser():\n",
    "    file_names = get_files_in_dir(directory)\n",
    "\n",
    "    for _id, _file in enumerate(file_names):\n",
    "        file_name = _file[6:]\n",
    "        file_data = get_text_file_data(_file)\n",
    "        data_body = \"\".join(file_data)\n",
    "\n",
    "        create_index(\"persian_poems_se1\")\n",
    "        doc_source = {\n",
    "            \"file_name\": file_name,\n",
    "            \"data\": data_body\n",
    "        }\n",
    "\n",
    "        yield {\n",
    "            \"_index\": \"persian_poems_se1\",\n",
    "            \"_id\": _id + 1,\n",
    "            \"_source\": doc_source\n",
    "        }\n",
    "\n",
    "\n",
    "def index_files():\n",
    "    try:\n",
    "        resp = helpers.bulk(\n",
    "            connect(),\n",
    "            text_files_parser()\n",
    "        )\n",
    "        print(\"Response:\", resp)\n",
    "        print(\"Response type:\", type(resp))\n",
    "    except Exception as err:\n",
    "        print(\"ERROR:\", err)\n",
    "\n",
    "\n",
    "def total_results(ap, precisions, recalls, f_measures):\n",
    "\n",
    "   # //total_map\n",
    "   map = 0\n",
    "   m_counter = 0\n",
    "   for a in ap:\n",
    "     map += a\n",
    "     m_counter +=1\n",
    "   try:\n",
    "    total_map = map/m_counter\n",
    "   except ZeroDivisionError:\n",
    "    total_map = 0\n",
    "\n",
    "   # //total_precision\n",
    "   pre = 0\n",
    "   p_counter = 0\n",
    "   for p in precisions:\n",
    "     pre += p\n",
    "     p_counter +=1\n",
    "   try:\n",
    "    total_precision = pre/p_counter\n",
    "   except ZeroDivisionError:\n",
    "    total_precision = 0\n",
    "\n",
    "   # //total_recall\n",
    "   rec = 0\n",
    "   r_counter = 0\n",
    "   for r in recalls:\n",
    "     rec += r\n",
    "     r_counter +=1\n",
    "   try:\n",
    "    total_recall = rec/r_counter\n",
    "   except ZeroDivisionError:\n",
    "    total_recall = 0\n",
    "\n",
    "   # //total F-Measure\n",
    "   fm = 0\n",
    "   f_counter = 0\n",
    "   for f in f_measures:\n",
    "     fm += f\n",
    "     f_counter +=1\n",
    "   try:\n",
    "    total_fmeasure = fm/f_counter\n",
    "   except ZeroDivisionError:\n",
    "    total_fmeasure = 0\n",
    "\n",
    "   print(\"Precision:\", total_precision)\n",
    "   print(\"Recall:\", total_recall)\n",
    "   print(\"F-measure:\", total_fmeasure)\n",
    "   print(\"MAP:\", total_map)\n",
    "\n",
    "\n",
    "def search_query():\n",
    "   es = connect()\n",
    "   file_names = get_files_in_dir(query_directory)\n",
    "\n",
    "   ap = []\n",
    "   precisions = []\n",
    "   recalls = []\n",
    "   f_measures = []\n",
    "\n",
    "   for name in file_names:\n",
    "     file_name = name[8:]\n",
    "     results = []\n",
    "     file_data = get_text_file_data(name)\n",
    "     data_body = \"\".join(file_data)\n",
    "\n",
    "     search_param = {\n",
    "    'query': {\n",
    "        'match': {\n",
    "            'data': data_body\n",
    "        }\n",
    "       }\n",
    "     }\n",
    "\n",
    "     print('Results for ' + file_name)\n",
    "     res = es.search(index=\"persian_poems_se1\", body=search_param)\n",
    "     # print(res)\n",
    "\n",
    "\n",
    "     print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "     for hit in res['hits']['hits']:\n",
    "         print(\"%(file_name)s\" % hit[\"_source\"])\n",
    "         results += [str(hit['_source']['file_name'])]\n",
    "\n",
    "\n",
    "     line_number = 0\n",
    "     with open(relevant_docs_path,\"r\" ,encoding=\"utf8\") as fp:\n",
    "\n",
    "        for line in fp:\n",
    "           if line.rstrip('\\n') == file_name:\n",
    "              fp.seek(0)\n",
    "              for i, sen in enumerate(fp):\n",
    "                 if i == line_number+1:\n",
    "                    rel_docs = sen.split()\n",
    "                    intersection_set = set.intersection(set(results), set(rel_docs))\n",
    "                    intersection_list = list(intersection_set)\n",
    "                    tp = len(intersection_list)\n",
    "                    fp = len(results) - len(intersection_list)\n",
    "                    fn= len(rel_docs) - len(intersection_list)\n",
    "\n",
    "                    try:\n",
    "                     precision = tp/(tp+fp)\n",
    "                     recall = tp/(tp+fn)\n",
    "                     f_measure = 2*precision*recall/(precision+recall)\n",
    "                    except ZeroDivisionError:\n",
    "                     precision = 0\n",
    "                     recall = 0\n",
    "                     f_measure = 0\n",
    "\n",
    "                    relative = 0\n",
    "                    counter = 1\n",
    "                    average_p = 0\n",
    "                    for result in results:\n",
    "                        if result in rel_docs:\n",
    "                            relative +=1\n",
    "                            average_p += (relative/counter)\n",
    "                        counter +=1\n",
    "                    try:\n",
    "                     ap.append(average_p/relative)\n",
    "                    except ZeroDivisionError:\n",
    "                     ap.append(0)\n",
    "\n",
    "                    precisions.append(precision)\n",
    "                    recalls.append(recall)\n",
    "                    f_measures.append(f_measure)\n",
    "              break\n",
    "           else:\n",
    "              line_number += 1\n",
    "   total_results(ap, precisions, recalls, f_measures)\n",
    "\n",
    "\n",
    "# All needed function calls are as follows:\n",
    "\n",
    "# //index files\n",
    "# index_files()\n",
    "\n",
    "# //search poems and print the total Precision,Recall,F_Measure and MAP of the SE\n",
    "# search_query()\n",
    "\n",
    "# //delete index\n",
    "# delete_index(['persian_poems_se1'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}